\documentclass[DM,lsstdraft,STS,toc]{lsstdoc}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{arydshln}

\input meta.tex


\def\product{Distributed Database}

\setDocCompact{true}

\title[STS for \product]{\product{} Software Test Specification}

\author{Fritz Mueller}
\setDocRef{\lsstDocType-\lsstDocNum}
\date{\vcsdate}
\setDocUpstreamLocation{\url{https://github.com/lsst/LDM-552}}

\makeglossaries
\input{aglossary.tex}

\setDocAbstract{
This document describes the detailed test specification for the \product{}.
}

\setDocChangeRecord{%
    \addtohist{}{2019-06-14}{Autogenerate from Jira.}{G.~Comoretto}
    \addtohist{}{2017-07-02}{Initial draft.}{F.~Mueller}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

This document specifies the test procedure for \product{}. \product{} is a distributed
shared-nothing RDBMS which will host \gls{LSST} catalogs.

\subsection{Objectives}
\label{sec:objectives}

This document builds on the description of \gls{LSST} \gls{Data Management}'s approach to
testing as described in \citeds{LDM-503} to describe the detailed tests that
will be performed on the \product{} as part of the verification of the \gls{DM} system.

It identifies test designs, test cases and procedures for the tests, and the
pass/fail criteria for each test.

\subsection{Scope}
\label{sec:scope}

This document describes the test procedures for the following components of
the \gls{LSST} system (as described in \citeds{LDM-148}):

\begin{itemize}
  \item{Parallel Distributed Database (\gls{Qserv})}
\end{itemize}

\subsection{Applicable Documents}
\label{sec:docs}

\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
  \citeds{LDM-135} & \gls{LSST} \gls{Qserv} Database Design \\
  \citeds{LDM-294} & \gls{LSST} \gls{DM} Organization \& Management \\
  \citeds{LDM-502} & The Measurement and \gls{Verification} of \gls{DM} Key Performance Metrics \\
  \citeds{LDM-503} & \gls{LSST} \gls{DM} Test Plan \\
  \citeds{LDM-555} & \gls{LSST} \gls{DM} Database Requirements \\
\end{tabular}

\subsection{References}
\label{sec:references
}
\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

\newpage
\section{Approach}
\label{sec:approach}

The approaches taken for the tests described here are:

\begin{itemize}

  \item{Ongoing inspection of design documents, code, and \gls{CI} system logs to verify that \product{} design
  and implementation meet \gls{DM} software quality standards in general, and requirements as expressed in
  \citeds{LDM-555} in particular;}

  \item{Ongoing deployment and continuous operation of \product{} in a Prototype \gls{Data Access Center}
  (\gls{PDAC}) in order to assess basic reliability, fitness for purpose, and integration with adjacent
  subsystems;}

  \item{Annual deployment of \product{} to test clusters, followed by synthesis and ingestion
  of test datasets and scripted performance/load/stress testing. The cluster size/capabilities and the
  scale of the synthetic test dataset are both evolved along a path toward anticipated \gls{LSST} operational
  scale.}

\end{itemize}

\subsection{Tasks and criteria}
\label{sec:tasks}

\product{} is a containerized, distributed, Linux application, which is deployed on machine clusters.
At the scales to be tested, these clusters are comprised of one to several head ("czar") nodes and
additionally on the order of tens to hundreds of shard ("worker") nodes, interconnected locally via a
high-performance network. Head and shard nodes are provisioned each with on the order of 10s of gigabytes of
RAM, and each with on the order of 10s of terabytes of locally attached storage.

Ongoing deployment, continuous operation, and integration tests are carried out on machines within the
Prototype \gls{Data Access Center} (\gls{PDAC}), a dedicated machine cluster physically located at \gls{NCSA}'s National
Peta-scale Compute Facility, maintained by \gls{NCSA} staff. Catalog datasets which are maintained within
the \gls{PDAC} \gls{Qserv} instance and which are used for this testing include, simultaneously:

\begin{itemize}
  \item{An \gls{LSST} \gls{stack} reprocessed version of the \gls{SDSS} \gls{Stripe 82} catalog (currently from Summer 2013 \citedsp{Document-15097}) ({\textasciitilde{}}10 \gls{TB});}
  \item{IRSA AllWISE and NEOWISE catalogs ({\textasciitilde{}}50 \gls{TB});}
  \item{An \gls{LSST} \gls{stack} reprocessed version of the \gls{HSC} catalog (scheduled; {\textasciitilde{}}50 \gls{TB}).}
\end{itemize}

Tasks required for these tests include periodic update of the software deployed on the \gls{PDAC},
periodic ingest of additional test datasets, and inter-operation with adjacent subsystems.  Uptime
is monitored cumulatively throughout these activities to gain quantitative insight into system
stability and reliability.

Scaling, load, and stress testing are carried out on an additional machine cluster located
CC-IN2P3 in Lyon, maintained by CC-IN2P3 staff. Scaling tests are run annually, by issuing
a representative mix of concurrent queries against a synthetic catalog while monitoring
average query execution times per query type.  The scaling test dataset size and query
concurrency level are increased each year on a glide path toward the full scale of \gls{Data Release} 1.

Tasks required for these tests include generation and ingest of each successive test dataset, and
execution of scripts which issue and monitor the suites of representative test queries.

\subsection{Features to be tested}
\label{sec:feat2test}

This version of the \product{} test specification addresses only basic product verification, basic
reliability, and performance/scale testing -- a bare minimum required to conduct ongoing development
and verify that \product{} remains on a realistic path towards meeting its most technically challenging
requirements: those related to successful operability at the scale that will be required by \gls{LSST}.

\subsection{Features not to be tested}
\label{sec:featnot2test}

Testing of the following are NOT YET COVERED in this specification:

\begin{itemize}
  \item{Fault-tolerance and disaster recovery;}
  \item{Schema evolution;}
  \item{Data ingest performance;}
  \item{Query reproducibility;}
  \item{Cross-match with external datasets.}
\end{itemize}

It is anticipated that test specifications and cases for all of the above will be developed
and added to future revisions of this document.

\subsection{Pass/fail criteria}
\label{sec:passfail}

The results of all tests will be assessed using the criteria described in \citeds{LDM-503} \S4.

\subsection{Suspension criteria and resumption requirements}
\label{suspension}

Refer to individual test cases where applicable.

\subsection{Naming convention}

With the introduction of the Jira Test Management, the following definitions have to be considered:

\begin{description}
  \item[LVV]{: Is the label for the ``LSST \gls{Verification} and \gls{Validation}'' project in Jira where all information regarding tests are managed.}
  \item[LVV-XXX]{: Are \gls{Verification} Elements, where XXX is the \gls{Verification} Element identifier.  Each \gls{Verification} Element is derived from a requirement and has at least one Test Case associated. There can be multiple \gls{Verification} Elements associated with a requirement.}
  \item[LVV-TYYY]{: Are Test Cases. Each Test Case is associated with a \gls{Verification} Element, where YYY is the Test Case identifier. There can be multiple test cases associated with a \gls{Verification} Element.}
\end{description}

All tests cases defined before the introduction of the Jira Test Management approach, are named according to the pattern \textsc{prod-scope-xx-yy} where:

\begin{description}[font=\normalfont\scshape]

  \item[prod]{The product code, per \citeds{LDM-294}. Relevant entries for this document are:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[qserv]{Qserv distributed database system}
    \end{description}
  }

  \item[scope]{The type of test being described:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[acp]{concerning acceptance testing}
      \item[bck]{concerning backup and restore testing}
      \item[fun]{concerning functional testing}
      \item[ins]{concerning installation testing}
      \item[int]{concerning integration testing}
      \item[itf]{concerning interface testing}
      \item[mnt]{concerning maintenance testing}
      \item[prf]{concerning performance testing}
      \item[reg]{concerning regression testing}
      \item[ver]{concerning verification testing}
    \end{description}
  }

  \item[xx]{Test design number (in increments of 10)}
  \item[yy]{Test case number (in increments of 5)}

\end{description}

\input{jira_docugen.tex}

\newpage
\appendix
\input{jira_docugen.appendix.tex}

\printglossaries

\end{document}
