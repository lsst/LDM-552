\documentclass[DM,STS,toc]{lsstdoc}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{arydshln}

\input meta.tex

\setcounter{tocdepth}{2}

\def\product{Distributed Database}

\setDocCompact{true}

\title[STS for \product]{\product{} Software Test Specification}

\author{Fritz Mueller}
\setDocRef{LDM-552}
\date{2019-07-25}
\setDocUpstreamLocation{\url{https://github.com/lsst/LDM-552}}


\setDocAbstract{
This document describes the detailed test specification for the \product{}.
}

\setDocChangeRecord{%
    \addtohist{}{2017-07-02}{Initial draft.}{F.~Mueller}
    \addtohist{1.0}{2019-07-25}{Document issued. Approved in RFC-612.}{G.~Comoretto}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

This document specifies the test procedure for \product{}. \product{} is a distributed
shared-nothing RDBMS which will host LSST catalogs.

\subsection{Objectives}
\label{sec:objectives}

This document builds on the description of LSST Data Management's approach to
testing as described in \citeds{LDM-503} to describe the detailed tests that
will be performed on the \product{} as part of the verification of the DM system.

It identifies test designs, test cases and procedures for the tests, and the
pass/fail criteria for each test.

\subsection{Scope}
\label{sec:scope}

This document describes the test procedures for the following components of
the LSST system (as described in \citeds{LDM-148}):

\begin{itemize}
  \item{Parallel Distributed Database (Qserv)}
\end{itemize}

\subsection{Applicable Documents}
\label{sec:docs}

\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
  \citeds{LDM-135} & LSST Qserv Database Design \\
  \citeds{LDM-294} & LSST DM Organization \& Management \\
  \citeds{LDM-502} & The Measurement and Verification of DM Key Performance Metrics \\
  \citeds{LDM-503} & LSST DM Test Plan \\
  \citeds{LDM-555} & LSST DM Database Requirements \\
\end{tabular}

\newpage
\section{Approach}
\label{sec:approach}

The approaches taken for the tests described here are:

\begin{itemize}

  \item{Ongoing inspection of design documents, code, and CI system logs to verify that \product{} design
  and implementation meet DM software quality standards in general, and requirements as expressed in
  \citeds{LDM-555} in particular;}

  \item{Ongoing deployment and continuous operation of \product{} in a Prototype Data Access Center
  (PDAC) in order to assess basic reliability, fitness for purpose, and integration with adjacent
  subsystems;}

  \item{Annual deployment of \product{} to test clusters, followed by synthesis and ingestion
  of test datasets and scripted performance/load/stress testing. The cluster size/capabilities and the
  scale of the synthetic test dataset are both evolved along a path toward anticipated LSST operational
  scale.}

\end{itemize}

\subsection{Tasks and criteria}
\label{sec:tasks}

\product{} is a containerized, distributed, Linux application, which is deployed on machine clusters.
At the scales to be tested, these clusters are comprised of one to several head ("czar") nodes and
additionally on the order of tens to hundreds of shard ("worker") nodes, interconnected locally via a
high-performance network. Head and shard nodes are provisioned each with on the order of 10s of gigabytes of
RAM, and each with on the order of 10s of terabytes of locally attached storage.

Ongoing deployment, continuous operation, and integration tests are carried out on machines within the
Prototype Data Access Center (PDAC), a dedicated machine cluster physically located at NCSA's National
Peta-scale Compute Facility, maintained by NCSA staff. Catalog datasets which are maintained within
the PDAC Qserv instance and which are used for this testing include, simultaneously:

\begin{itemize}
  \item{An LSST stack reprocessed version of the SDSS Stripe 82 catalog (currently from Summer 2013 \citedsp{Document-15097}) ({\textasciitilde{}}10 TB);}
  \item{IRSA AllWISE and NEOWISE catalogs ({\textasciitilde{}}50 TB);}
  \item{An LSST stack reprocessed version of the HSC catalog (scheduled; {\textasciitilde{}}50 TB).}
\end{itemize}

Tasks required for these tests include periodic update of the software deployed on the PDAC,
periodic ingest of additional test datasets, and inter-operation with adjacent subsystems.  Uptime
is monitored cumulatively throughout these activities to gain quantitative insight into system
stability and reliability.

Scaling, load, and stress testing are carried out on an additional machine cluster located
CC-IN2P3 in Lyon, maintained by CC-IN2P3 staff. Scaling tests are run annually, by issuing
a representative mix of concurrent queries against a synthetic catalog while monitoring
average query execution times per query type.  The scaling test dataset size and query
concurrency level are increased each year on a glide path toward the full scale of Data Release 1.

Tasks required for these tests include generation and ingest of each successive test dataset, and
execution of scripts which issue and monitor the suites of representative test queries.

\subsection{Features to be tested}
\label{sec:feat2test}

This version of the \product{} test specification addresses only basic product verification, basic
reliability, and performance/scale testing -- a bare minimum required to conduct ongoing development
and verify that \product{} remains on a realistic path towards meeting its most technically challenging
requirements: those related to successful operability at the scale that will be required by LSST.


\subsubsection{Performances}

In order to ensure that QSERV is able to meet the performance, specific test cases have been designed.
These test cases will be executed each year, in order to denmonstrate that query performances are as described in the following table.


    \begin{tabular}{lccccccc}\hline
      \multicolumn{2}{c}{\textbf{Query Class}}
        &\textbf{2015}&\textbf{2016}&\textbf{2017}&\textbf{2018}&\textbf{2019}&\textbf{2020}\\\hline
      \multicolumn{2}{c}{Dataset size, relative to DR-1} 
                              & 10\% & 20\% & 30\% & 50\% & 75\% & 100\% \\ \hline
      \multirow{2}{*}{\textbf{LV}}
        &\textbf{\# queries}  & 50   & 60   & 70   & 80   & 90   & 100 \\%\cline{2-8}
        &\textbf{time (sec)}  & 10   & 10   & 10   & 10   & 10   &  10 \\\hline
      \multirow{2}{*}{\textbf{FTSObj}}
        &\textbf{\# queries}  &  3 &  4 &  8 & 12 & 16 &  20 \\%\cline{2-8}
        &\textbf{time (hours)}& 12 &  1 &  1 &  1 &  1 &   1 \\\hline
      \multirow{2}{*}{\textbf{FTSSrc}}
        &\textbf{\# queries}  &  1 &  1 &  2 &  3 &  4 &   5 \\%\cline{2-8}
        &\textbf{time (hours)}& 12 & 12 & 12 & 12 & 12 &  12 \\\hline
      \multirow{2}{*}{\textbf{FTSFSrc}}
        &\textbf{\# queries}  &    &  1 &  2 &  3 &  4 &   5 \\%\cline{2-8}
        &\textbf{time (hours)}&    & 12 & 12 & 12 & 12 &  12 \\\hline
      \multirow{2}{*}{\textbf{joinObjSrc}}
        &\textbf{\# queries}  &  1 &  2 &  4 &  6 &  8 &  10 \\%\cline{2-8}
        &\textbf{time (hours)}& 12 & 12 & 12 & 12 & 12 &  12 \\\hline
      \multirow{2}{*}{\textbf{joinObjFSrc}}
        &\textbf{\# queries}  &    &  1 &  2 &  3 &  4 &   5 \\%\cline{2-8}
        &\textbf{time (hours)}&    & 12 & 12 & 12 & 12 &  12 \\\hline
      \multirow{2}{*}{\textbf{nearN}}
        &\textbf{\# queries}  &    &  1 &  2 &  3 &  4 &   5 \\%\cline{2-8}
        &\textbf{time (hours)}&    &  1 &  1 &  1 &  1 &   1 \\\hline
    \end{tabular}


\subsection{Features not to be tested}
\label{sec:featnot2test}

Testing of the following are NOT YET COVERED in this specification:

\begin{itemize}
  \item{Fault-tolerance and disaster recovery;}
  \item{Schema evolution;}
  \item{Data ingest performance;}
  \item{Query reproducibility;}
  \item{Cross-match with external datasets.}
\end{itemize}

It is anticipated that test specifications and cases for all of the above will be developed
and added to future revisions of this document.

\subsection{Pass/fail criteria}
\label{sec:passfail}

The results of all tests will be assessed using the criteria described in \citeds{LDM-503} \S4.

\subsection{Suspension criteria and resumption requirements}
\label{suspension}

Refer to individual test cases where applicable.

\subsection{Naming convention}

With the introduction of the Jira Test Management, the following definitions have to be considered:

\begin{description}
  \item[LVV]{: Is the label for the ``LSST Verification and Validation'' project in Jira where all information regarding tests are managed.}
  \item[LVV-XXX]{: Are Verification Elements, where XXX is the Verification Element identifier.  Each Verification Element is derived from a requirement and has at least one Test Case associated. There can be multiple Verification Elements associated with a requirement.}
  \item[LVV-TYYY]{: Are Test Cases. Each Test Case is associated with a Verification Element, where YYY is the Test Case identifier. There can be multiple test cases associated with a Verification Element.}
\end{description}

\newpage
\input{jira_docugen.tex}

\newpage
\appendix
\input{jira_docugen.appendix.tex}

\newpage
\section{References\label{sect:references}}
\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads,local.bib}

\newpage
\section{Acronyms \label{sect:acronyms}} % include acronyms.tex generated by the generateAcronyms.py (in texmf/scripts)
\input{acronyms}

\end{document}
